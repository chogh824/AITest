{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff97a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: ipywidgets in ./.local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.local/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.local/lib/python3.10/site-packages (from ipywidgets) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.local/lib/python3.10/site-packages (from ipywidgets) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: decorator in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.1)\n",
      "Requirement already satisfied: stack-data in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.11)\n",
      "Requirement already satisfied: six>=1.5 in ./.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch accelerate ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c735aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda3fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e644b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (from bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6826f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in ./.local/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in ./.local/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.local/lib/python3.10/site-packages (from peft) (2.4.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (from peft) (4.45.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.local/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in ./.local/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in ./.local/lib/python3.10/site-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e3e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.__file__)  # ì„¤ì¹˜ ê²½ë¡œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2432464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d76985d2b294cd79d2959b393f4396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m\n\u001b[1;32m     54\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     55\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     torch_empty_cache_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 12. íŠ¸ë ˆì´ë„ˆ ì„¤ì •\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 13. ëª¨ë¸ í•™ìŠµ ì‹œì‘\u001b[39;00m\n\u001b[1;32m     81\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:554\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    553\u001b[0m ):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:802\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 802\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 1. JSON íŒŒì¼ ë¡œë“œ\n",
    "with open(\"traindata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_data(example):\n",
    "    input_text = f\"### ìê¸°ì†Œê°œì„œ\\nì´ë¦„: {example['input']['name']}\\nì§€ì› ë¶„ì•¼: {example['input']['applied_position']}\\nì§€ì› ë™ê¸°: {example['input']['reason_for_applying']}\\nìê¸°ì†Œê°œ: {example['input']['self_introduction']}\\n\\n### ë©´ì ‘ ì§ˆë¬¸:\\n\"\n",
    "    target_text = \"\\n\".join(example[\"target\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "# 3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "processed_data = [preprocess_data(ex) for ex in data]\n",
    "\n",
    "# 4. Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [ex[\"input_text\"] for ex in processed_data],\n",
    "    \"target_text\": [ex[\"target_text\"] for ex in processed_data]\n",
    "})\n",
    "\n",
    "# 5. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° pad_token ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # eos_tokenì„ pad_tokenìœ¼ë¡œ ì„¤ì •\n",
    "\n",
    "# 6. í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ ì •ì˜\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# 7. ë°ì´í„° í† í¬ë‚˜ì´ì§•\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# 8. ë°ì´í„°ì…‹ ë¶„í•  (í•™ìŠµ/í‰ê°€ìš©)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 9. ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T\")\n",
    "\n",
    "# 10. GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 11. í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ì´ë¯¸ ìµœì†Œë¡œ ì„¤ì •ë¨\n",
    "    per_device_eval_batch_size=1,  \n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=64,  # ê¸°ìš¸ê¸° ëˆ„ì  ìŠ¤í…ì„ ë” ëŠ˜ë ¤ ë©”ëª¨ë¦¬ ì‚¬ìš© ì ˆì•½\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile=False,\n",
    "    torch_empty_cache_steps=50\n",
    ")\n",
    "# 12. íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 13. ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# 14. í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"./trained_tinyllama\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea5be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Interview Questions:\n",
      "### ìê¸°ì†Œê°œì„œ\n",
      "ì´ë¦„: ê¹€ë¯¼ì¤€\n",
      "ì§€ì› ë¶„ì•¼: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì\n",
      "ì§€ì› ë™ê¸°: ì‚¬ìš©ì ê²½í—˜ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ëŠ” í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ë¶„ì•¼ì—ì„œ ì €ì˜ ì—­ëŸ‰ì„ ë°œíœ˜í•˜ê³  ì‹¶ì–´ ì§€ì›í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ì›¹ ê¸°ìˆ ê³¼ ë””ìì¸ íŠ¸ë Œë“œì— ëŒ€í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·€ì‚¬ì˜ ì œí’ˆì— ê°€ì¹˜ë¥¼ ë”í•˜ê³ ì í•©ë‹ˆë‹¤\n",
      "ìê¸°ì†Œê°œ: ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì ê¹€ë¯¼ì¤€ì…ë‹ˆë‹¤. ëŒ€í•™ ì‹œì ˆë¶€í„° ì›¹ ê°œë°œì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì ¸ì™”ìœ¼ë©°, ì»´í“¨í„°ê³µí•™ê³¼ì—ì„œ ì›¹ ê¸°ìˆ  ê´€ë ¨ ê³¼ëª©ë“¤ì„ ìˆ˜ê°•í•˜ë©° ê¸°ì´ˆë¥¼ ë‹¤ì¡ŒìŠµë‹ˆë‹¤. 3í•™ë…„ ë•ŒëŠ” ë™ì•„ë¦¬ í™œë™ì„ í†µí•´ íŒ€ í”„ë¡œì íŠ¸ë¡œ ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì›¹ì‚¬ì´íŠ¸ë¥¼ êµ¬ì¶•í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ì—ì„œ ì €ëŠ” React.jsë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì˜€ìœ¼ë©°, Reduxë¥¼ í†µí•´ ìƒíƒœ ê´€ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë°±ì—”ë“œ ê°œë°œìì™€ì˜ í˜‘ì—…ì„ í†µí•´ RESTful APIë¥¼ ì—°ë™í•˜ê³ , Axiosë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í†µì‹ ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤. ì¡¸ì—… í›„ì—ëŠ” ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ 2ë…„ê°„ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìë¡œ ê·¼ë¬´í•˜ë©°, ëª¨ë°”ì¼ ìµœì í™”ì™€ ë°˜ì‘í˜• ë””ìì¸ì— ëŒ€í•œ ê²½í—˜ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤. ì´ ê¸°ê°„ ë™ì•ˆ TypeScriptë¥¼ ë„ì…í•˜ì—¬ ì½”ë“œì˜ ì•ˆì •ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, Webpackê³¼ Babelì„ í™œìš©í•˜ì—¬ ë²ˆë“¤ë§ê³¼ íŠ¸ëœìŠ¤íŒŒì¼ë§ ê³¼ì •ì„ ìµœì í™”í•˜ì˜€ìŠµë‹ˆë‹¤. ì‚¬ìš©ì ê²½í—˜(UX)ì— ëŒ€í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì—¬ ì „í™˜ìœ¨ì„ 15% í–¥ìƒì‹œí‚¨ ì„±ê³¼ë„ ìˆìŠµë‹ˆë‹¤. ì €ëŠ” í•­ìƒ ìƒˆë¡œìš´ ê¸°ìˆ ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ìµœê·¼ì—ëŠ” Next.jsì™€ GraphQLì„ ê³µë¶€í•˜ì—¬ ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ê³¼ íš¨ìœ¨ì ì¸ ë°ì´í„° í˜ì¹­ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. ê·€ì‚¬ì—ì„œ ì´ëŸ¬í•œ ê²½í—˜ê³¼ ì—´ì •ì„ ë°”íƒ•ìœ¼ë¡œ ë”ìš± ë°œì „ëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë° ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ë©´ì ‘ ì§ˆë¬¸:\n",
      "- **What is your biggest challenge?** :\n",
      "  ë‚´ê°€ ë³¸ êµìœ¡ë‹¨ì—ì„œëŠ” ì£¼ì¥ í¬ìŠ¤íŠ¸ ì¤‘ í•œ ë‚´ìš©ë§Œ ë‚˜ì˜¤ëŠ” ê²ƒì„ ìœ„í•´ ë§ˆìŒì´ ë§ì´ ì°¨ì§€í•˜ê³ ì”ë‹ˆ ê·¸ ë‚´å®¹ì— ë¹ ë¥´ê² ë‹¤. (ì˜êµ­ì–´)\n",
      "    - **Why did you choose this topic to write about?**ï¼šë‚´ê°€ \"Essentialism\"ë¥¼ ì½ì€ ë’¤ì— ë‚´ íšŒì‚¬ì— íˆ¬ëª…í•˜ê±°ë‚˜ ì¬ë¯¸ì¡í•˜ê¸¸ ì¢‹ì•„í–” ë‚´ê°€ <NAME>ì˜ ì±…ì„ read í—ˆìš©í–ë‹ˆ ë¯¿ì„ ì •ë„ë¡œ ë‚´ì¼ ë¨¹ê³  ë‘ ëª« ë„£ê¸°ë¥¼ ê¾¸ì¤˜ì•‰ë‹¤. <br/><br/>\n",
      "    ë‚´ê°€ â€œEssentialsâ€ë¥¼ ì •ë¦¬í•˜ë©´ì„œ ì¡°ê¸ˆ ëŠë‚€ ê±´ ìºë¦­í„° ë“± ì„¤ëª… ë°©ì‹ì´ ì¡°ì‹®ì´ ê°™ì€ ë‚´ì—­ì´ ë‚¨ì•„ì˜ ì–»ê³  ê·¸ëŸ¼ ë‚´ê²°ì  ì •ì˜ë¥¼ í•˜ê³  â€˜ë‹¹ì‹ ì˜ ë‚œì´ë„â€™ë¥¼ ê¸°ë¡í•˜ë©´ ì•ìœµê¸€ì— ì¶”ì²œí•˜ê³¤ í•œë‹¤.<br/>ë„ˆë¬´ ë´! (ì˜í™”)\n",
      "        - **What are the most important skills for a developer in today's world?** (ë„¤ì´ë²‰ í‘œ 400px)\n",
      "            - **Python ìŠ¤í„°ë”©** : Python íŠœí† ë¦¬ì–¼ì„ ë“¤ì´ê³  ë‚´ê¸° ìœ„í•œ ê¸´ ê³„ê¸° íƒˆì¶œ ì‹œê°ì œì˜ ìŠ¤íŠ¸ë¦¼ì´ ë“¤ì–´ê°€ê² ì‰½ê²¨ ë‹¬ë¼ ì“°ê¸° ë¸”ë¡œê·¸ë¥¼ ë§Œë“¤ê¸° ì‹œí– ë‹ˆë‹¤.(ì˜í™” 2ë¶ˆ)\n",
      "                - **HTML / CSS / JavaScript / NodeJS / Express / React.Js / Typescript / Netlify Cms / Nginx / Docker / AWS Lambda / Sass / PostgreSQL / MySQL / MongoDB / Cloudinary / Google Analytics / GitHub Pages / Gatsby / Vercel / Amazon DynamoDB / Amazon Kinesis / Elastic Beanstalk / Heroku / AWS Amplify / AWS AppSync / AWS Glue / AWS Identity & Access Management / Amazon Route 53 / Amazon VPC / Amazon EC2 / Amazon ECS / Amazon Fargate / AWS CodeCommit / AWS CLI / AWS SDK / AWS IoT Core / AWS WAF / AWS Xray / AWS Network Security Group / AWS Load Balancer / AWS Virtual Private Cloud / AWS Firewall Manager / AWS Secrets Manager / Azure Active Directory / Azure AD Premium / Azure SQL Database / Azure Cosmos DB / Azure Data Lake Storage / Azure Databricks / Azure Machine Learning / Azure Synapse / Azure Logic Apps / Azure Functions / Microsoft Power Platform / Microsoft Flow / Azure Monitor / Azure DevOps / Azure Container Registry / Azure Image Builder / Azure Arc / Azure Site Recovery / Azure\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_path = \"./trained_tinyllama\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 2. í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ ë¬¸ì¥ ì •ì˜\n",
    "input_text = \"### ìê¸°ì†Œê°œì„œ\\nì´ë¦„: ê¹€ë¯¼ì¤€\\nì§€ì› ë¶„ì•¼: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì\\nì§€ì› ë™ê¸°: ì‚¬ìš©ì ê²½í—˜ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ëŠ” í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ë¶„ì•¼ì—ì„œ ì €ì˜ ì—­ëŸ‰ì„ ë°œíœ˜í•˜ê³  ì‹¶ì–´ ì§€ì›í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ì›¹ ê¸°ìˆ ê³¼ ë””ìì¸ íŠ¸ë Œë“œì— ëŒ€í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·€ì‚¬ì˜ ì œí’ˆì— ê°€ì¹˜ë¥¼ ë”í•˜ê³ ì í•©ë‹ˆë‹¤\\nìê¸°ì†Œê°œ: ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì ê¹€ë¯¼ì¤€ì…ë‹ˆë‹¤. ëŒ€í•™ ì‹œì ˆë¶€í„° ì›¹ ê°œë°œì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì ¸ì™”ìœ¼ë©°, ì»´í“¨í„°ê³µí•™ê³¼ì—ì„œ ì›¹ ê¸°ìˆ  ê´€ë ¨ ê³¼ëª©ë“¤ì„ ìˆ˜ê°•í•˜ë©° ê¸°ì´ˆë¥¼ ë‹¤ì¡ŒìŠµë‹ˆë‹¤. 3í•™ë…„ ë•ŒëŠ” ë™ì•„ë¦¬ í™œë™ì„ í†µí•´ íŒ€ í”„ë¡œì íŠ¸ë¡œ ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì›¹ì‚¬ì´íŠ¸ë¥¼ êµ¬ì¶•í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ì—ì„œ ì €ëŠ” React.jsë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì˜€ìœ¼ë©°, Reduxë¥¼ í†µí•´ ìƒíƒœ ê´€ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë°±ì—”ë“œ ê°œë°œìì™€ì˜ í˜‘ì—…ì„ í†µí•´ RESTful APIë¥¼ ì—°ë™í•˜ê³ , Axiosë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í†µì‹ ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤. ì¡¸ì—… í›„ì—ëŠ” ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ 2ë…„ê°„ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìë¡œ ê·¼ë¬´í•˜ë©°, ëª¨ë°”ì¼ ìµœì í™”ì™€ ë°˜ì‘í˜• ë””ìì¸ì— ëŒ€í•œ ê²½í—˜ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤. ì´ ê¸°ê°„ ë™ì•ˆ TypeScriptë¥¼ ë„ì…í•˜ì—¬ ì½”ë“œì˜ ì•ˆì •ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í–¥ìƒì‹œì¼°ìœ¼ë©°, Webpackê³¼ Babelì„ í™œìš©í•˜ì—¬ ë²ˆë“¤ë§ê³¼ íŠ¸ëœìŠ¤íŒŒì¼ë§ ê³¼ì •ì„ ìµœì í™”í•˜ì˜€ìŠµë‹ˆë‹¤. ì‚¬ìš©ì ê²½í—˜(UX)ì— ëŒ€í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì—¬ ì „í™˜ìœ¨ì„ 15% í–¥ìƒì‹œí‚¨ ì„±ê³¼ë„ ìˆìŠµë‹ˆë‹¤. ì €ëŠ” í•­ìƒ ìƒˆë¡œìš´ ê¸°ìˆ ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ìµœê·¼ì—ëŠ” Next.jsì™€ GraphQLì„ ê³µë¶€í•˜ì—¬ ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ê³¼ íš¨ìœ¨ì ì¸ ë°ì´í„° í˜ì¹­ì— ëŒ€í•œ ì´í•´ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. ê·€ì‚¬ì—ì„œ ì´ëŸ¬í•œ ê²½í—˜ê³¼ ì—´ì •ì„ ë°”íƒ•ìœ¼ë¡œ ë”ìš± ë°œì „ëœ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë° ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.\\n\\n### ë©´ì ‘ ì§ˆë¬¸:\\n\"\n",
    "\n",
    "# 3. ì…ë ¥ ë¬¸ì¥ í† í¬ë‚˜ì´ì§•\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# ëª¨ë¸ì„ í†µí•´ ì§ˆë¬¸ ìƒì„± (íŒŒë¼ë¯¸í„° ì¬ì„¤ì •)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=2000,           # ìµœëŒ€ ê¸¸ì´ë¥¼ 2000ìœ¼ë¡œ ì œí•œ\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=3,   # 3ê·¸ë¨ ë°˜ë³µ ë°©ì§€\n",
    "    temperature=0.5,          # ë” ë³´ìˆ˜ì ì¸ ìƒ˜í”Œë§\n",
    "    top_p=0.85,               # top-p ìƒ˜í”Œë§ìœ¼ë¡œ ì¡°ê¸ˆ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ ì„ íƒ\n",
    "    do_sample=True,           # ìƒ˜í”Œë§ í™œì„±í™”\n",
    "    repetition_penalty=1.3    # ë°˜ë³µ íŒ¨í„´ ë°©ì§€\n",
    ")\n",
    "\n",
    "# ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Generated Interview Questions:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41ade990",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m device_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.h\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# ì£¼ìš” ë ˆì´ì–´ëŠ” GPUì— í• ë‹¹\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# lm_headëŠ” CPUì— í• ë‹¹\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.wte\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# ì„ë² ë”© ë ˆì´ì–´ëŠ” GPUì— í• ë‹¹\u001b[39;00m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 2. ëª¨ë¸ ë¡œë“œ\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ì–‘ìí™” ì„¤ì • ì „ë‹¬\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# ëª…ì‹œì ì¸ device_map ì„¤ì •\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ì§ì ‘ ì œê³µëœ í† í° ì‚¬ìš©\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# LoRA ì„¤ì •\u001b[39;00m\n\u001b[1;32m     33\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     34\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM,  \u001b[38;5;66;03m# Causal Language Modeling\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# í•™ìŠµ ëª¨ë“œ\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "token = \"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\"\n",
    "\n",
    "# 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ìœ„í•œ BitsAndBytesConfig ìƒì„±\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# ëª…ì‹œì ì¸ device_map ì„¤ì • (CPUì™€ GPU ê°„ ëª¨ë“ˆ ë¶„ë°°)\n",
    "device_map = {\n",
    "    \"transformer.h\": \"cuda:0\",  # ì£¼ìš” ë ˆì´ì–´ëŠ” GPUì— í• ë‹¹\n",
    "    \"lm_head\": \"cpu\",           # lm_headëŠ” CPUì— í• ë‹¹\n",
    "    \"transformer.wte\": \"cuda:0\" # ì„ë² ë”© ë ˆì´ì–´ëŠ” GPUì— í• ë‹¹\n",
    "}\n",
    "\n",
    "# 2. ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # ì–‘ìí™” ì„¤ì • ì „ë‹¬\n",
    "    device_map=device_map,           # ëª…ì‹œì ì¸ device_map ì„¤ì •\n",
    "    use_auth_token=token  # ì§ì ‘ ì œê³µëœ í† í° ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal Language Modeling\n",
    "    inference_mode=False,  # í•™ìŠµ ëª¨ë“œ\n",
    "    r=8,  # ë­í¬\n",
    "    lora_alpha=32,  # ë¡œë¼ ì•ŒíŒŒ\n",
    "    lora_dropout=0.1  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    ")\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ëª¨ë¸ ì ìš©\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”í•  ê²½ìš° eos_tokenì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒˆë¡œ ì •ì˜)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # eos_tokenì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì‚¬ìš©\n",
    "\n",
    "# 4. ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "with open(\"traindata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 5. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_data(example):\n",
    "    input_text = f\"### ìê¸°ì†Œê°œì„œ\\nì´ë¦„: {example['input']['name']}\\nì§€ì› ë¶„ì•¼: {example['input']['applied_position']}\\nì§€ì› ë™ê¸°: {example['input']['reason_for_applying']}\\nìê¸°ì†Œê°œ: {example['input']['self_introduction']}\\n\\n### ë©´ì ‘ ì§ˆë¬¸:\\n\"\n",
    "    target_text = \"\\n\".join(example[\"target\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "processed_data = [preprocess_data(ex) for ex in data]\n",
    "\n",
    "# 6. Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [ex[\"input_text\"] for ex in processed_data],\n",
    "    \"target_text\": [ex[\"target_text\"] for ex in processed_data]\n",
    "})\n",
    "\n",
    "# 7. í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ ì •ì˜\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# 8. ë°ì´í„°ì…‹ ë¶„í•  (í•™ìŠµ/í‰ê°€)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 9. í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ì„¤ì •\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=8,  # ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì‘ìœ¼ë¯€ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ë” ë§ì´ í•¨\n",
    ")\n",
    "\n",
    "# 10. íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# 11. ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# 12. ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"./fine_tuned_kollm_interview_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebf1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
