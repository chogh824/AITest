{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff97a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: ipywidgets in ./.local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.local/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.local/lib/python3.10/site-packages (from ipywidgets) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.local/lib/python3.10/site-packages (from ipywidgets) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: decorator in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.1)\n",
      "Requirement already satisfied: stack-data in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.11)\n",
      "Requirement already satisfied: six>=1.5 in ./.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch accelerate ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c735aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda3fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e644b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (from bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6826f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in ./.local/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in ./.local/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.local/lib/python3.10/site-packages (from peft) (2.4.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (from peft) (4.45.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.local/lib/python3.10/site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in ./.local/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in ./.local/lib/python3.10/site-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e3e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.__file__)  # 설치 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2432464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d76985d2b294cd79d2959b393f4396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m\n\u001b[1;32m     54\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     55\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     torch_empty_cache_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 12. 트레이너 설정\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 13. 모델 학습 시작\u001b[39;00m\n\u001b[1;32m     81\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:554\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    553\u001b[0m ):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:802\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 802\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 1. JSON 파일 로드\n",
    "with open(\"traindata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. 데이터 전처리 함수\n",
    "def preprocess_data(example):\n",
    "    input_text = f\"### 자기소개서\\n이름: {example['input']['name']}\\n지원 분야: {example['input']['applied_position']}\\n지원 동기: {example['input']['reason_for_applying']}\\n자기소개: {example['input']['self_introduction']}\\n\\n### 면접 질문:\\n\"\n",
    "    target_text = \"\\n\".join(example[\"target\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "# 3. 데이터 전처리\n",
    "processed_data = [preprocess_data(ex) for ex in data]\n",
    "\n",
    "# 4. Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [ex[\"input_text\"] for ex in processed_data],\n",
    "    \"target_text\": [ex[\"target_text\"] for ex in processed_data]\n",
    "})\n",
    "\n",
    "# 5. 토크나이저 로드 및 pad_token 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # eos_token을 pad_token으로 설정\n",
    "\n",
    "# 6. 토크나이징 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# 7. 데이터 토크나이징\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# 8. 데이터셋 분할 (학습/평가용)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 9. 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T\")\n",
    "\n",
    "# 10. GPU 메모리 정리\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 11. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # 배치 사이즈는 이미 최소로 설정됨\n",
    "    per_device_eval_batch_size=1,  \n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=64,  # 기울기 누적 스텝을 더 늘려 메모리 사용 절약\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile=False,\n",
    "    torch_empty_cache_steps=50\n",
    ")\n",
    "# 12. 트레이너 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 13. 모델 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 14. 학습된 모델 저장\n",
    "trainer.save_model(\"./trained_tinyllama\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea5be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Interview Questions:\n",
      "### 자기소개서\n",
      "이름: 김민준\n",
      "지원 분야: 프론트엔드 개발자\n",
      "지원 동기: 사용자 경험을 최우선으로 고려하는 프론트엔드 개발 분야에서 저의 역량을 발휘하고 싶어 지원하게 되었습니다. 최신 웹 기술과 디자인 트렌드에 대한 이해를 바탕으로 귀사의 제품에 가치를 더하고자 합니다\n",
      "자기소개: 안녕하세요, 프론트엔드 개발자 김민준입니다. 대학 시절부터 웹 개발에 깊은 관심을 가져왔으며, 컴퓨터공학과에서 웹 기술 관련 과목들을 수강하며 기초를 다졌습니다. 3학년 때는 동아리 활동을 통해 팀 프로젝트로 온라인 쇼핑몰 웹사이트를 구축한 경험이 있습니다. 이 프로젝트에서 저는 React.js를 활용하여 사용자 친화적인 인터페이스를 구현하였으며, Redux를 통해 상태 관리를 효율적으로 처리했습니다. 또한, 백엔드 개발자와의 협업을 통해 RESTful API를 연동하고, Axios를 사용하여 데이터 통신을 구현하였습니다. 졸업 후에는 스타트업에서 2년간 프론트엔드 개발자로 근무하며, 모바일 최적화와 반응형 디자인에 대한 경험을 쌓았습니다. 이 기간 동안 TypeScript를 도입하여 코드의 안정성과 유지보수성을 향상시켰으며, Webpack과 Babel을 활용하여 번들링과 트랜스파일링 과정을 최적화하였습니다. 사용자 경험(UX)에 대한 이해를 바탕으로 A/B 테스트를 진행하여 전환율을 15% 향상시킨 성과도 있습니다. 저는 항상 새로운 기술에 관심을 가지고 있으며, 최근에는 Next.js와 GraphQL을 공부하여 서버 사이드 렌더링과 효율적인 데이터 페칭에 대한 이해를 높였습니다. 귀사에서 이러한 경험과 열정을 바탕으로 더욱 발전된 서비스를 제공하는 데 기여하고 싶습니다.\n",
      "\n",
      "### 면접 질문:\n",
      "- **What is your biggest challenge?** :\n",
      "  내가 본 교육단에서는 주장 포스트 중 한 내용만 나오는 것을 위해 마음이 많이 차지하고잔니 그 내容에 빠르겠다. (영국어)\n",
      "    - **Why did you choose this topic to write about?**：내가 \"Essentialism\"를 읽은 뒤에 내 회사에 투명하거나 재미잡하길 좋아햔 내가 <NAME>의 책을 read 허용햐니 믿을 정도로 내일 먹고 두 몫 넣기를 꾸줘앉다. <br/><br/>\n",
      "    내가 “Essentials”를 정리하면서 조금 느낀 건 캐릭터 등 설명 방식이 조싮이 같은 내역이 남아잘 얻고 그럼 내결점 정의를 하고 ‘당신의 난이도’를 기록하면 앞융글에 추천하곤 한다.<br/>너무 봐! (영화)\n",
      "        - **What are the most important skills for a developer in today's world?** (네이벉 표 400px)\n",
      "            - **Python 스터딩** : Python 튜토리얼을 들이고 내기 위한 긴 계기 탈출 시각제의 스트림이 들어가겐 쉽겨 달라 쓰기 블로그를 만들기 시햠니다.(영화 2불)\n",
      "                - **HTML / CSS / JavaScript / NodeJS / Express / React.Js / Typescript / Netlify Cms / Nginx / Docker / AWS Lambda / Sass / PostgreSQL / MySQL / MongoDB / Cloudinary / Google Analytics / GitHub Pages / Gatsby / Vercel / Amazon DynamoDB / Amazon Kinesis / Elastic Beanstalk / Heroku / AWS Amplify / AWS AppSync / AWS Glue / AWS Identity & Access Management / Amazon Route 53 / Amazon VPC / Amazon EC2 / Amazon ECS / Amazon Fargate / AWS CodeCommit / AWS CLI / AWS SDK / AWS IoT Core / AWS WAF / AWS Xray / AWS Network Security Group / AWS Load Balancer / AWS Virtual Private Cloud / AWS Firewall Manager / AWS Secrets Manager / Azure Active Directory / Azure AD Premium / Azure SQL Database / Azure Cosmos DB / Azure Data Lake Storage / Azure Databricks / Azure Machine Learning / Azure Synapse / Azure Logic Apps / Azure Functions / Microsoft Power Platform / Microsoft Flow / Azure Monitor / Azure DevOps / Azure Container Registry / Azure Image Builder / Azure Arc / Azure Site Recovery / Azure\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. 학습된 모델과 토크나이저 로드\n",
    "model_path = \"./trained_tinyllama\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 2. 테스트용 입력 문장 정의\n",
    "input_text = \"### 자기소개서\\n이름: 김민준\\n지원 분야: 프론트엔드 개발자\\n지원 동기: 사용자 경험을 최우선으로 고려하는 프론트엔드 개발 분야에서 저의 역량을 발휘하고 싶어 지원하게 되었습니다. 최신 웹 기술과 디자인 트렌드에 대한 이해를 바탕으로 귀사의 제품에 가치를 더하고자 합니다\\n자기소개: 안녕하세요, 프론트엔드 개발자 김민준입니다. 대학 시절부터 웹 개발에 깊은 관심을 가져왔으며, 컴퓨터공학과에서 웹 기술 관련 과목들을 수강하며 기초를 다졌습니다. 3학년 때는 동아리 활동을 통해 팀 프로젝트로 온라인 쇼핑몰 웹사이트를 구축한 경험이 있습니다. 이 프로젝트에서 저는 React.js를 활용하여 사용자 친화적인 인터페이스를 구현하였으며, Redux를 통해 상태 관리를 효율적으로 처리했습니다. 또한, 백엔드 개발자와의 협업을 통해 RESTful API를 연동하고, Axios를 사용하여 데이터 통신을 구현하였습니다. 졸업 후에는 스타트업에서 2년간 프론트엔드 개발자로 근무하며, 모바일 최적화와 반응형 디자인에 대한 경험을 쌓았습니다. 이 기간 동안 TypeScript를 도입하여 코드의 안정성과 유지보수성을 향상시켰으며, Webpack과 Babel을 활용하여 번들링과 트랜스파일링 과정을 최적화하였습니다. 사용자 경험(UX)에 대한 이해를 바탕으로 A/B 테스트를 진행하여 전환율을 15% 향상시킨 성과도 있습니다. 저는 항상 새로운 기술에 관심을 가지고 있으며, 최근에는 Next.js와 GraphQL을 공부하여 서버 사이드 렌더링과 효율적인 데이터 페칭에 대한 이해를 높였습니다. 귀사에서 이러한 경험과 열정을 바탕으로 더욱 발전된 서비스를 제공하는 데 기여하고 싶습니다.\\n\\n### 면접 질문:\\n\"\n",
    "\n",
    "# 3. 입력 문장 토크나이징\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 통해 질문 생성 (파라미터 재설정)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=2000,           # 최대 길이를 2000으로 제한\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=3,   # 3그램 반복 방지\n",
    "    temperature=0.5,          # 더 보수적인 샘플링\n",
    "    top_p=0.85,               # top-p 샘플링으로 조금 더 신뢰할 수 있는 단어 선택\n",
    "    do_sample=True,           # 샘플링 활성화\n",
    "    repetition_penalty=1.3    # 반복 패턴 방지\n",
    ")\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Generated Interview Questions:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41ade990",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m device_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.h\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 주요 레이어는 GPU에 할당\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# lm_head는 CPU에 할당\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.wte\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# 임베딩 레이어는 GPU에 할당\u001b[39;00m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 2. 모델 로드\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 양자화 설정 전달\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 명시적인 device_map 설정\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 직접 제공된 토큰 사용\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# LoRA 설정\u001b[39;00m\n\u001b[1;32m     33\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     34\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM,  \u001b[38;5;66;03m# Causal Language Modeling\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 학습 모드\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# 드롭아웃 비율\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 1. 모델과 토크나이저 설정\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "token = \"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\"\n",
    "\n",
    "# 4비트 양자화를 위한 BitsAndBytesConfig 생성\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# 명시적인 device_map 설정 (CPU와 GPU 간 모듈 분배)\n",
    "device_map = {\n",
    "    \"transformer.h\": \"cuda:0\",  # 주요 레이어는 GPU에 할당\n",
    "    \"lm_head\": \"cpu\",           # lm_head는 CPU에 할당\n",
    "    \"transformer.wte\": \"cuda:0\" # 임베딩 레이어는 GPU에 할당\n",
    "}\n",
    "\n",
    "# 2. 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # 양자화 설정 전달\n",
    "    device_map=device_map,           # 명시적인 device_map 설정\n",
    "    use_auth_token=token  # 직접 제공된 토큰 사용\n",
    ")\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal Language Modeling\n",
    "    inference_mode=False,  # 학습 모드\n",
    "    r=8,  # 랭크\n",
    "    lora_alpha=32,  # 로라 알파\n",
    "    lora_dropout=0.1  # 드롭아웃 비율\n",
    ")\n",
    "\n",
    "# LoRA 어댑터 모델 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3. 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "# 패딩 토큰 설정 (필요할 경우 eos_token을 사용하거나 새로 정의)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # eos_token을 패딩 토큰으로 사용\n",
    "\n",
    "# 4. 데이터셋 로드\n",
    "with open(\"traindata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 5. 데이터 전처리 함수\n",
    "def preprocess_data(example):\n",
    "    input_text = f\"### 자기소개서\\n이름: {example['input']['name']}\\n지원 분야: {example['input']['applied_position']}\\n지원 동기: {example['input']['reason_for_applying']}\\n자기소개: {example['input']['self_introduction']}\\n\\n### 면접 질문:\\n\"\n",
    "    target_text = \"\\n\".join(example[\"target\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "processed_data = [preprocess_data(ex) for ex in data]\n",
    "\n",
    "# 6. Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [ex[\"input_text\"] for ex in processed_data],\n",
    "    \"target_text\": [ex[\"target_text\"] for ex in processed_data]\n",
    "})\n",
    "\n",
    "# 7. 토크나이징 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# 8. 데이터셋 분할 (학습/평가)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 9. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # 배치 사이즈를 작게 설정\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=8,  # 배치 사이즈가 작으므로 그래디언트 누적을 더 많이 함\n",
    ")\n",
    "\n",
    "# 10. 트레이너 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# 11. 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 12. 모델 저장\n",
    "trainer.save_model(\"./fine_tuned_kollm_interview_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebf1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
